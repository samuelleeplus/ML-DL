{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SU_24774_24218_Code.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfK6ZcddSp3B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CS 412 - Machine Learning Project\n",
        "# Authors:\n",
        "# Onat Kaya\n",
        "# Samuel Lee\n",
        "\n",
        "#!pip install -q keras\n",
        "import keras\n",
        "import tensorflow\n",
        "\n",
        "#lets preprocess the data\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy \n",
        "\n",
        "from sklearn.preprocessing import normalize \n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "def preprocessing(fileName): # does preprocessing on the file specified at the parameter.\n",
        "  d = pd.read_csv(fileName)\n",
        "  #print(d.isnull().sum())\n",
        "  d['home_ownership'] = pd.factorize(d.home_ownership)[0] + 1\n",
        "  d['grade'] = pd.factorize(d.grade)[0] + 1\n",
        "  d[\"term\"] = d['term'].map({' 60 months': 1, ' 36 months':0})\n",
        "  d['verification_status'] = pd.factorize(d.verification_status)[0] + 1\n",
        "  d['purpose'] = pd.factorize(d.purpose)[0] + 1\n",
        "  d['application_type'] = pd.factorize(d.application_type)[0] + 1\n",
        "  d['addr_state'] = pd.factorize(d.addr_state)[0] + 1\n",
        "  d['zip_code'] = pd.factorize(d.zip_code)[0] + 1\n",
        "  d.emp_length.unique()\n",
        "  d[\"emp_length\"] = d['emp_length'].map({'< 1 year':0 , '3 years':3, '2 years':2, '10+ years':10 , '6 years':6,\n",
        "       '7 years':7, '8 years':8, '1 year':1, '4 years':4, '5 years':5, '9 years':9})\n",
        "\n",
        "  #df.isnull().sum()\n",
        "  d = d.drop(columns='emp_title')\n",
        "  d = d.drop(columns='earliest_cr_line')\n",
        "  #normalizing the data \n",
        "  float_array = np.array(d['loan_amnt'])\n",
        "  dtemp = pd.DataFrame(float_array, columns=['loan_amnt']) \n",
        "\n",
        "  #scaling\n",
        "  x = dtemp.values #returns a numpy array\n",
        "  min_max_scaler = MinMaxScaler()\n",
        "  x_scaled = min_max_scaler.fit_transform(x)\n",
        "  dtemp = pd.DataFrame(x_scaled)\n",
        "  #renaming the columns \n",
        "  dtemp.columns = ['loan_amnt']\n",
        "  d['loan_amnt'] = dtemp['loan_amnt']\n",
        "\n",
        "  #normalizing the annual_inc\n",
        "  #create new pandas dataframe\n",
        "  annual_inc = np.array(d['annual_inc'])\n",
        "  min_max_scaler =  MinMaxScaler()\n",
        "  dtemp = pd.DataFrame(annual_inc, columns=['annual_inc']) \n",
        "  #scaling\n",
        "  x = dtemp.values #returns a numpy array\n",
        "  min_max_scaler =  MinMaxScaler()\n",
        "  x_scaled = min_max_scaler.fit_transform(x)\n",
        "  dtemp = pd.DataFrame(x_scaled)\n",
        "  #renaming the columns \n",
        "  dtemp.columns = ['annual_inc']\n",
        "  d['annual_inc'] = dtemp['annual_inc']\n",
        "\n",
        "  #normalizing the revol_bal\n",
        "  #create new pandas dataframe\n",
        "  revol_bal = np.array(d['revol_bal'])\n",
        "  min_max_scaler =  MinMaxScaler()\n",
        "  dtemp = pd.DataFrame(revol_bal, columns=['revol_bal']) \n",
        "\n",
        "  #scaling\n",
        "  x = dtemp.values #returns a numpy array\n",
        "  min_max_scaler =  MinMaxScaler()\n",
        "  x_scaled = min_max_scaler.fit_transform(x)\n",
        "  dtemp = pd.DataFrame(x_scaled)\n",
        "  #renaming the columns \n",
        "  dtemp.columns = ['revol_bal']\n",
        "  d['revol_bal'] = dtemp['revol_bal']\n",
        "\n",
        "  #normalizing the tot_cur_bal\n",
        "  #create new pandas dataframe\n",
        "  tot_cur_bal = np.array(d['tot_cur_bal'])\n",
        "  min_max_scaler =  MinMaxScaler()\n",
        "  dtemp = pd.DataFrame(tot_cur_bal, columns=['tot_cur_bal']) \n",
        "\n",
        "  #scaling\n",
        "  x = dtemp.values #returns a numpy array\n",
        "  min_max_scaler =  MinMaxScaler()\n",
        "  x_scaled = min_max_scaler.fit_transform(x)\n",
        "  dtemp = pd.DataFrame(x_scaled)\n",
        "  #renaming the columns \n",
        "  dtemp.columns = ['tot_cur_bal']\n",
        "  d['tot_cur_bal'] = dtemp['tot_cur_bal']\n",
        "\n",
        "  #normalizing the avg_cur_bal\n",
        "  #create new pandas dataframe\n",
        "  avg_cur_bal = np.array(d['avg_cur_bal'])\n",
        "  min_max_scaler =  MinMaxScaler()\n",
        "  dtemp = pd.DataFrame(avg_cur_bal, columns=['avg_cur_bal']) \n",
        "\n",
        "  #scaling\n",
        "  x = dtemp.values #returns a numpy array\n",
        "  min_max_scaler =  MinMaxScaler()\n",
        "  x_scaled = min_max_scaler.fit_transform(x)\n",
        "  dtemp = pd.DataFrame(x_scaled)\n",
        "  #renaming the columns \n",
        "  dtemp.columns = ['avg_cur_bal']\n",
        "  d['avg_cur_bal'] = dtemp['avg_cur_bal']\n",
        "\n",
        "  #normalizing the bc_open_to_buy\n",
        "  #create new pandas dataframe\n",
        "  bc_open_to_buy = np.array(d['bc_open_to_buy'])\n",
        "  min_max_scaler =  MinMaxScaler()\n",
        "  dtemp = pd.DataFrame(bc_open_to_buy, columns=['bc_open_to_buy']) \n",
        "\n",
        "  #scaling\n",
        "  x = dtemp.values #returns a numpy array\n",
        "  min_max_scaler =  MinMaxScaler()\n",
        "  x_scaled = min_max_scaler.fit_transform(x)\n",
        "  dtemp = pd.DataFrame(x_scaled)\n",
        "  #renaming the columns \n",
        "  dtemp.columns = ['bc_open_to_buy']\n",
        "\n",
        "  d['bc_open_to_buy'] = dtemp['bc_open_to_buy']\n",
        "\n",
        "    #normalizing the tot_hi_cred_lim\n",
        "  #create new pandas dataframe\n",
        "  tot_hi_cred_lim = np.array(d['tot_hi_cred_lim'])\n",
        "  min_max_scaler =  MinMaxScaler()\n",
        "  dtemp = pd.DataFrame(tot_hi_cred_lim, columns=['tot_hi_cred_lim']) \n",
        "\n",
        "  #scaling\n",
        "  x = dtemp.values #returns a numpy array\n",
        "  min_max_scaler =  MinMaxScaler()\n",
        "  x_scaled = min_max_scaler.fit_transform(x)\n",
        "  dtemp = pd.DataFrame(x_scaled)\n",
        "  #renaming the columns \n",
        "  dtemp.columns = ['tot_hi_cred_lim']\n",
        "  d['tot_hi_cred_lim'] = dtemp['tot_hi_cred_lim']\n",
        "\n",
        "\n",
        "  #normalizing the total_bal_ex_mort\n",
        "  #create new pandas dataframe\n",
        "  total_bal_ex_mort = np.array(d['total_bal_ex_mort'])\n",
        "  min_max_scaler =  MinMaxScaler()\n",
        "  dtemp = pd.DataFrame(total_bal_ex_mort, columns=['total_bal_ex_mort']) \n",
        "\n",
        "  #scaling\n",
        "  x = dtemp.values #returns a numpy array\n",
        "  min_max_scaler =  MinMaxScaler()\n",
        "  x_scaled = min_max_scaler.fit_transform(x)\n",
        "  dtemp = pd.DataFrame(x_scaled)\n",
        "  #renaming the columns \n",
        "  dtemp.columns = ['total_bal_ex_mort']\n",
        "  d['total_bal_ex_mort'] = dtemp['total_bal_ex_mort']\n",
        "\n",
        "\n",
        "  #normalizing the total_il_high_credit_limit\n",
        "  #create new pandas dataframe\n",
        "  total_il_high_credit_limit = np.array(d['total_il_high_credit_limit'])\n",
        "  min_max_scaler =  MinMaxScaler()\n",
        "  dtemp = pd.DataFrame(total_il_high_credit_limit, columns=['total_il_high_credit_limit']) \n",
        "\n",
        "  #scaling\n",
        "  x = dtemp.values #returns a numpy array\n",
        "  min_max_scaler =  MinMaxScaler()\n",
        "  x_scaled = min_max_scaler.fit_transform(x)\n",
        "  dtemp = pd.DataFrame(x_scaled)\n",
        "  #renaming the columns \n",
        "  dtemp.columns = ['total_il_high_credit_limit']\n",
        "  d['total_il_high_credit_limit'] = dtemp['total_il_high_credit_limit']\n",
        "\n",
        "\n",
        "  #normalizing the mo_sin_old_rev_tl_op\n",
        "  #create new pandas dataframe\n",
        "  mo_sin_old_rev_tl_op = np.array(d['mo_sin_old_rev_tl_op'])\n",
        "  min_max_scaler =  MinMaxScaler()\n",
        "  dtemp = pd.DataFrame(mo_sin_old_rev_tl_op, columns=['mo_sin_old_rev_tl_op']) \n",
        "\n",
        "  #scaling\n",
        "  x = dtemp.values #returns a numpy array\n",
        "  min_max_scaler =  MinMaxScaler()\n",
        "  x_scaled = min_max_scaler.fit_transform(x)\n",
        "  dtemp = pd.DataFrame(x_scaled)\n",
        "  #renaming the columns \n",
        "  dtemp.columns = ['mo_sin_old_rev_tl_op']\n",
        "  d['mo_sin_old_rev_tl_op'] = dtemp['mo_sin_old_rev_tl_op']\n",
        "\n",
        "  return d\n",
        "\n",
        "\n",
        "# Applying preprocessing\n",
        "train = preprocessing(\"loansTrain.csv\")\n",
        "test = preprocessing(\"loansTest.csv\")\n",
        "\n",
        "for col in train.columns: # Filling up N/A values with the mean value.\n",
        "  if train[col].isnull().values.any():\n",
        "    train[col].fillna(train[col].mean(), inplace=True)\n",
        "\n",
        "\n",
        "test = test.drop(columns='ID') \n",
        "\n",
        "for col in test.columns:  # Filling up N/A values with the mean value.\n",
        "  if test[col].isnull().values.any():\n",
        "    test[col].fillna(test[col].mean(), inplace=True)\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "y = train.loan_status\n",
        "x = train.drop('loan_status',axis=1)\n",
        "\n",
        "X_train,X_test,y_train,y_test= train_test_split(x,y,test_size=0.2) # splitting our data\n",
        "\n",
        "clf_NN = MLPClassifier(solver='adam',hidden_layer_sizes=(20,)) # we are going to use the MLP classifier. We are using 20 hidden layers.\n",
        "clf_NN.fit(X_train,y_train) # training our data.\n",
        "\n",
        "predict_NN = clf_NN.predict(test) # making predictions about loansTest.csv file\n",
        "predictproba_NN = clf_NN.predict_proba(test)[:,1]\n",
        "\n",
        "\n",
        "prediction_size = len(predict_NN)\n",
        "id_arr = []\n",
        "\n",
        "for i in range(0, prediction_size):\n",
        "    id_arr.append(i)\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Writing the prediction results to our .csv file\n",
        "\n",
        "import csv\n",
        "from itertools import zip_longest\n",
        "d = [id_arr, predict_NN]\n",
        "export_data = zip_longest(*d, fillvalue = '')\n",
        "with open('SU_24774_24218_Submission.csv', 'w', encoding=\"ISO-8859-1\", newline='') as myfile:\n",
        "      wr = csv.writer(myfile)\n",
        "      wr.writerow((\"ID\", \"loan_status\"))\n",
        "      wr.writerows(export_data)\n",
        "myfile.close()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}